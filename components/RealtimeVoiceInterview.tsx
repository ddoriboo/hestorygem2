'use client'

import { useState, useEffect, useRef, useCallback } from 'react'

interface RealtimeVoiceInterviewProps {
  sessionNumber: number
  onConversationSave: (question: string, answer: string) => Promise<void>
}

interface Conversation {
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
  audioComplete?: boolean
}

export default function RealtimeVoiceInterview({ sessionNumber, onConversationSave }: RealtimeVoiceInterviewProps) {
  const [isConnected, setIsConnected] = useState(false)
  const [isRecording, setIsRecording] = useState(false)
  const [conversations, setConversations] = useState<Conversation[]>([])
  const [connectionStatus, setConnectionStatus] = useState('ì—°ê²° ì¤€ë¹„ ì¤‘...')
  const [currentTranscript, setCurrentTranscript] = useState('')
  const [isAISpeaking, setIsAISpeaking] = useState(false)
  const [lastTimestamp, setLastTimestamp] = useState(0)

  // Audio refs
  const localStreamRef = useRef<MediaStream | null>(null)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const pollingIntervalRef = useRef<NodeJS.Timeout | null>(null)

  useEffect(() => {
    return () => {
      disconnect()
    }
  }, [])

  const connectToRealtime = useCallback(async () => {
    try {
      setConnectionStatus('OpenAI Realtime APIì— ì—°ê²° ì¤‘...')

      // ì„¸ì…˜ ì‹œì‘
      const sessionResponse = await fetch('/api/interview/realtime-polling', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ 
          sessionNumber, 
          action: 'start' 
        })
      })

      if (!sessionResponse.ok) {
        throw new Error('ì„¸ì…˜ ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
      }

      const { sessionId } = await sessionResponse.json()
      console.log('ì„¸ì…˜ ì‹œì‘ë¨:', sessionId)

      // ë§ˆì´í¬ ì„¤ì •
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            sampleRate: 24000,
            channelCount: 1
          }
        })
        localStreamRef.current = stream
        console.log('ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ì„¤ì • ì™„ë£Œ')
      } catch (error) {
        console.error('ë§ˆì´í¬ ì•¡ì„¸ìŠ¤ ì‹¤íŒ¨:', error)
        setConnectionStatus('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.')
        return
      }

      setIsConnected(true)
      setConnectionStatus('ìŒì„± ì¸í„°ë·° ì¤€ë¹„ ì™„ë£Œ')

      // ë©”ì‹œì§€ í´ë§ ì‹œì‘
      startPolling()

      // ì˜¤ë””ì˜¤ ë…¹ìŒ ì„¤ì •
      setupAudioRecording()

    } catch (error) {
      console.error('Realtime ì—°ê²° ì˜¤ë¥˜:', error)
      setConnectionStatus(`ì—°ê²° ì‹¤íŒ¨: ${error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`)
    }
  }, [sessionNumber])

  const startPolling = useCallback(() => {
    const poll = async () => {
      if (!isConnected) return

      try {
        const response = await fetch(`/api/interview/realtime-polling?sessionNumber=${sessionNumber}&lastTimestamp=${lastTimestamp}`)
        if (!response.ok) return

        const data = await response.json()
        
        if (!data.connected) {
          setIsConnected(false)
          setConnectionStatus('ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤.')
          return
        }

        // ìƒˆë¡œìš´ ë©”ì‹œì§€ ì²˜ë¦¬
        data.messages.forEach((message: Record<string, unknown>) => {
          handleRealtimeEvent(message)
        })

        if (data.lastTimestamp) {
          setLastTimestamp(data.lastTimestamp)
        }
      } catch (error) {
        console.error('í´ë§ ì˜¤ë¥˜:', error)
      }
    }

    // 200msë§ˆë‹¤ í´ë§
    pollingIntervalRef.current = setInterval(poll, 200)
  }, [sessionNumber, lastTimestamp, isConnected])

  const setupAudioRecording = useCallback(async () => {
    if (!localStreamRef.current) return

    try {
      // AudioContext ì„¤ì •
      const audioContext = new AudioContext({ sampleRate: 24000 })
      audioContextRef.current = audioContext

      // MediaRecorder ì„¤ì •
      const mediaRecorder = new MediaRecorder(localStreamRef.current, {
        mimeType: 'audio/webm;codecs=opus'
      })
      mediaRecorderRef.current = mediaRecorder

      let audioChunks: Blob[] = []

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunks.push(event.data)
        }
      }

      mediaRecorder.onstop = async () => {
        if (audioChunks.length > 0) {
          const audioBlob = new Blob(audioChunks, { type: 'audio/webm;codecs=opus' })
          await sendAudioData(audioBlob)
          audioChunks = []
        }
        setIsRecording(false)
      }

      mediaRecorder.onstart = () => {
        setIsRecording(true)
        audioChunks = []
      }

    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ ë…¹ìŒ ì„¤ì • ì˜¤ë¥˜:', error)
    }
  }, [])

  const sendAudioData = useCallback(async (audioBlob: Blob) => {
    try {
      // Convert to base64
      const arrayBuffer = await audioBlob.arrayBuffer()
      const audioData = new Uint8Array(arrayBuffer)
      const base64Audio = btoa(String.fromCharCode(...audioData))

      // Send audio data
      await fetch('/api/interview/realtime-polling', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          sessionNumber,
          action: 'send',
          data: {
            type: 'input_audio_buffer.append',
            audio: base64Audio
          }
        })
      })

      // Commit audio buffer
      await fetch('/api/interview/realtime-polling', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          sessionNumber,
          action: 'send',
          data: {
            type: 'input_audio_buffer.commit'
          }
        })
      })

      // Request response
      await fetch('/api/interview/realtime-polling', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          sessionNumber,
          action: 'send',
          data: {
            type: 'response.create'
          }
        })
      })

    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ ë°ì´í„° ì „ì†¡ ì˜¤ë¥˜:', error)
    }
  }, [sessionNumber])

  const handleRealtimeEvent = useCallback((data: Record<string, unknown>) => {
    try {
      console.log('Realtime ì´ë²¤íŠ¸:', data.type, data)

      switch (data.type) {
        case 'session.created':
          console.log('ì„¸ì…˜ ìƒì„±ë¨:', data.session)
          break

        case 'session.updated':
          console.log('ì„¸ì…˜ ì—…ë°ì´íŠ¸ë¨')
          break

        case 'response.created':
          console.log('ì‘ë‹µ ìƒì„± ì‹œì‘')
          setIsAISpeaking(true)
          break

        case 'response.output_item.added':
          console.log('ì¶œë ¥ ì•„ì´í…œ ì¶”ê°€ë¨:', data.item)
          break

        case 'response.content_part.added':
          console.log('ì½˜í…ì¸  íŒŒíŠ¸ ì¶”ê°€ë¨:', data.part)
          break

        case 'response.audio_transcript.delta':
          // AI ì‘ë‹µ í…ìŠ¤íŠ¸ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
          setCurrentTranscript(prev => prev + (data.delta as string))
          break

        case 'response.audio_transcript.done':
          // AI ì‘ë‹µ ì™„ë£Œ
          const assistantMessage: Conversation = {
            role: 'assistant',
            content: data.transcript as string,
            timestamp: new Date(),
            audioComplete: true
          }
          setConversations(prev => [...prev, assistantMessage])
          setCurrentTranscript('')
          setIsAISpeaking(false)
          break

        case 'response.audio.delta':
          // AI ì‘ë‹µ ì˜¤ë””ì˜¤ ì‹¤ì‹œê°„ ì¬ìƒ (base64 PCM16)
          if (data.delta) {
            playAudioChunk(data.delta as string)
          }
          break

        case 'input_audio_buffer.speech_started':
          console.log('ì‚¬ìš©ì ìŒì„± ê°ì§€ ì‹œì‘')
          setIsRecording(true)
          break

        case 'input_audio_buffer.speech_stopped':
          console.log('ì‚¬ìš©ì ìŒì„± ê°ì§€ ì¤‘ì§€')
          setIsRecording(false)
          break

        case 'conversation.item.input_audio_transcription.completed':
          // ì‚¬ìš©ì ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜ ì™„ë£Œ
          const userMessage: Conversation = {
            role: 'user',
            content: data.transcript as string,
            timestamp: new Date()
          }
          setConversations(prev => {
            const newConversations = [...prev, userMessage]
            
            // ëŒ€í™” ì €ì¥ (ì§ˆë¬¸-ë‹µë³€ ìŒ)
            if (newConversations.length >= 2) {
              const lastAssistant = newConversations[newConversations.length - 2]
              if (lastAssistant && lastAssistant.role === 'assistant') {
                onConversationSave(lastAssistant.content, userMessage.content)
              }
            }
            
            return newConversations
          })
          break

        case 'response.done':
          console.log('ì‘ë‹µ ì™„ë£Œ:', data.response)
          setIsAISpeaking(false)
          break

        case 'error':
          console.error('Realtime API ì˜¤ë¥˜:', data)
          setConnectionStatus(`ì˜¤ë¥˜: ${(data.error as Record<string, unknown>)?.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`)
          break

        default:
          console.log('ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì´ë²¤íŠ¸:', data.type)
      }
    } catch (error) {
      console.error('ì´ë²¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜:', error)
    }
  }, [onConversationSave])

  const playAudioChunk = useCallback(async (audioData: string) => {
    try {
      if (!audioContextRef.current) return

      // Base64 ë””ì½”ë”©
      const binaryString = atob(audioData)
      const len = binaryString.length
      const bytes = new Uint8Array(len)
      
      for (let i = 0; i < len; i++) {
        bytes[i] = binaryString.charCodeAt(i)
      }

      // PCM16 to Float32Array
      const pcmData = new Int16Array(bytes.buffer)
      const floatData = new Float32Array(pcmData.length)
      
      for (let i = 0; i < pcmData.length; i++) {
        floatData[i] = pcmData[i] / 32768.0
      }

      // AudioBuffer ìƒì„± ë° ì¬ìƒ
      const audioBuffer = audioContextRef.current.createBuffer(1, floatData.length, 24000)
      audioBuffer.getChannelData(0).set(floatData)
      
      const source = audioContextRef.current.createBufferSource()
      source.buffer = audioBuffer
      source.connect(audioContextRef.current.destination)
      source.start()
      
    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜:', error)
    }
  }, [])

  const disconnect = useCallback(async () => {
    console.log('ì—°ê²° í•´ì œ ì¤‘...')

    // í´ë§ ì¤‘ì§€
    if (pollingIntervalRef.current) {
      clearInterval(pollingIntervalRef.current)
      pollingIntervalRef.current = null
    }

    // ë…¹ìŒ ì¤‘ì§€
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop()
    }

    // ìŠ¤íŠ¸ë¦¼ í•´ì œ
    if (localStreamRef.current) {
      localStreamRef.current.getTracks().forEach(track => track.stop())
      localStreamRef.current = null
    }

    // AudioContext í•´ì œ
    if (audioContextRef.current) {
      await audioContextRef.current.close()
      audioContextRef.current = null
    }

    // ì„œë²„ ì„¸ì…˜ ì¢…ë£Œ
    try {
      await fetch('/api/interview/realtime-polling', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          sessionNumber,
          action: 'close'
        })
      })
    } catch (error) {
      console.error('ì„¸ì…˜ ì¢…ë£Œ ì˜¤ë¥˜:', error)
    }

    setIsConnected(false)
    setIsRecording(false)
    setIsAISpeaking(false)
    setConnectionStatus('ì—°ê²° í•´ì œë¨')
    setLastTimestamp(0)
  }, [sessionNumber])

  const interruptAI = useCallback(async () => {
    if (isAISpeaking) {
      try {
        await fetch('/api/interview/realtime-polling', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            sessionNumber,
            action: 'send',
            data: {
              type: 'response.cancel'
            }
          })
        })
        setIsAISpeaking(false)
      } catch (error) {
        console.error('AI ì¤‘ë‹¨ ì˜¤ë¥˜:', error)
      }
    }
  }, [isAISpeaking, sessionNumber])

  const startRecording = useCallback(() => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'inactive') {
      mediaRecorderRef.current.start()
    }
  }, [])

  const stopRecording = useCallback(() => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop()
    }
  }, [])

  return (
    <div className="bg-white rounded-lg shadow-lg p-6">
      <div className="mb-6">
        <h3 className="text-xl font-semibold text-gray-800 mb-2">ğŸ¤ ì‹¤ì‹œê°„ ìŒì„± ì¸í„°ë·°</h3>
        <p className="text-gray-600">{connectionStatus}</p>
      </div>

      {/* ì—°ê²° ë²„íŠ¼ */}
      <div className="flex justify-center mb-6 space-x-4">
        {!isConnected ? (
          <button
            onClick={connectToRealtime}
            className="px-6 py-3 bg-blue-600 text-white rounded-lg hover:bg-blue-700 transition font-medium"
          >
            ğŸ¤ ìŒì„± ì¸í„°ë·° ì‹œì‘
          </button>
        ) : (
          <>
            <button
              onClick={disconnect}
              className="px-6 py-3 bg-red-600 text-white rounded-lg hover:bg-red-700 transition font-medium"
            >
              ğŸ›‘ ì¸í„°ë·° ì¢…ë£Œ
            </button>
            {isAISpeaking && (
              <button
                onClick={interruptAI}
                className="px-6 py-3 bg-yellow-600 text-white rounded-lg hover:bg-yellow-700 transition font-medium"
              >
                â¸ï¸ AI ì¤‘ë‹¨
              </button>
            )}
          </>
        )}
      </div>

      {/* ìˆ˜ë™ ë…¹ìŒ ë²„íŠ¼ */}
      {isConnected && (
        <div className="flex justify-center mb-6 space-x-4">
          {!isRecording ? (
            <button
              onClick={startRecording}
              className="px-4 py-2 bg-green-600 text-white rounded-lg hover:bg-green-700 transition"
            >
              ğŸ™ï¸ ìˆ˜ë™ ë…¹ìŒ ì‹œì‘
            </button>
          ) : (
            <button
              onClick={stopRecording}
              className="px-4 py-2 bg-red-600 text-white rounded-lg hover:bg-red-700 transition"
            >
              â¹ï¸ ë…¹ìŒ ì •ì§€
            </button>
          )}
        </div>
      )}

      {/* ìŒì„± ìƒíƒœ í‘œì‹œ */}
      {isConnected && (
        <div className="text-center mb-6">
          <div className="space-y-2">
            <div className={`inline-flex items-center px-4 py-2 rounded-full ${
              isRecording ? 'bg-red-100 text-red-800' : 'bg-gray-100 text-gray-600'
            }`}>
              <div className={`w-3 h-3 rounded-full mr-2 ${
                isRecording ? 'bg-red-500 animate-pulse' : 'bg-gray-400'
              }`} />
              {isRecording ? 'ğŸ¤ ë§ì”€í•˜ê³  ê³„ì‹­ë‹ˆë‹¤...' : 'ğŸ§ AI ì¸í„°ë·°ì–´ê°€ ë“£ê³  ìˆìŠµë‹ˆë‹¤'}
            </div>
            
            {isAISpeaking && (
              <div className="inline-flex items-center px-4 py-2 rounded-full bg-blue-100 text-blue-800">
                <div className="w-3 h-3 rounded-full mr-2 bg-blue-500 animate-pulse" />
                ğŸ—£ï¸ AIê°€ ì‘ë‹µí•˜ê³  ìˆìŠµë‹ˆë‹¤...
              </div>
            )}
          </div>
        </div>
      )}

      {/* í˜„ì¬ AI ì‘ë‹µ í‘œì‹œ */}
      {currentTranscript && (
        <div className="mb-4 p-4 bg-blue-50 rounded-lg border-l-4 border-blue-400">
          <p className="text-blue-800 font-medium">AI (ì‹¤ì‹œê°„):</p>
          <p className="text-blue-700 mt-1">{currentTranscript}</p>
        </div>
      )}

      {/* ëŒ€í™” ê¸°ë¡ */}
      <div className="max-h-96 overflow-y-auto space-y-4">
        {conversations.map((conv, index) => (
          <div
            key={index}
            className={`p-4 rounded-lg ${
              conv.role === 'assistant' 
                ? 'bg-blue-50 border-l-4 border-blue-400' 
                : 'bg-green-50 border-l-4 border-green-400'
            }`}
          >
            <div className="flex justify-between items-start mb-2">
              <span className={`font-medium ${
                conv.role === 'assistant' ? 'text-blue-800' : 'text-green-800'
              }`}>
                {conv.role === 'assistant' ? 'ğŸ¤– AI ì¸í„°ë·°ì–´' : 'ğŸ‘¤ ì•„ë²„ë‹˜'}
                {conv.audioComplete && conv.role === 'assistant' && (
                  <span className="ml-2 text-xs bg-blue-200 text-blue-700 px-2 py-1 rounded">ìŒì„± ì™„ë£Œ</span>
                )}
              </span>
              <span className="text-xs text-gray-500">
                {conv.timestamp.toLocaleTimeString()}
              </span>
            </div>
            <p className="text-gray-800 whitespace-pre-wrap">{conv.content}</p>
          </div>
        ))}

        {conversations.length === 0 && isConnected && (
          <div className="text-center text-gray-500 py-8">
            <p>ì¸í„°ë·°ê°€ ê³§ ì‹œì‘ë©ë‹ˆë‹¤...</p>
            <p className="text-sm mt-2">ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•˜ê³  AIì˜ ì§ˆë¬¸ì„ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.</p>
          </div>
        )}
      </div>

      {/* ì‚¬ìš© íŒ */}
      {isConnected && (
        <div className="mt-4 p-3 bg-gray-50 rounded-lg text-sm text-gray-600">
          <h4 className="font-medium text-gray-800 mb-2">ğŸ’¡ ì‚¬ìš© íŒ:</h4>
          <ul className="space-y-1 text-xs">
            <li>â€¢ AIê°€ ì§ˆë¬¸ì„ í•˜ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”</li>
            <li>â€¢ ìŒì„± ê°ì§€ê°€ ìë™ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤</li>
            <li>â€¢ ìˆ˜ë™ ë…¹ìŒ ë²„íŠ¼ìœ¼ë¡œë„ ë…¹ìŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤</li>
            <li>â€¢ AIê°€ ë§í•˜ëŠ” ì¤‘ì—ë„ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤</li>
            <li>â€¢ ë‹µë³€ì´ ê¸¸ì–´ë„ ê´œì°®ìŠµë‹ˆë‹¤ - í¸ì•ˆí•˜ê²Œ ì´ì•¼ê¸°í•´ì£¼ì„¸ìš”</li>
          </ul>
        </div>
      )}
    </div>
  )
}