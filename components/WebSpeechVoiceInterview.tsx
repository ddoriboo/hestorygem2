'use client'

import { useState, useEffect, useRef, useCallback } from 'react'

interface WebSpeechVoiceInterviewProps {
  sessionNumber: number
  onConversationSave: (question: string, answer: string) => Promise<void>
}

interface Conversation {
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
  isListening?: boolean
  isSpeaking?: boolean
}

// Web Speech API íƒ€ì… ì •ì˜
declare global {
  interface Window {
    SpeechRecognition: any
    webkitSpeechRecognition: any
  }
}

export default function WebSpeechVoiceInterview({ sessionNumber, onConversationSave }: WebSpeechVoiceInterviewProps) {
  const [isConnected, setIsConnected] = useState(false)
  const [isListening, setIsListening] = useState(false)
  const [isSpeaking, setIsSpeaking] = useState(false)
  const [conversations, setConversations] = useState<Conversation[]>([])
  const [connectionStatus, setConnectionStatus] = useState('ìŒì„± ì¸í„°ë·° ì¤€ë¹„ë¨')
  const [currentTranscript, setCurrentTranscript] = useState('')
  const [interimTranscript, setInterimTranscript] = useState('')

  // Speech API refs
  const recognitionRef = useRef<any>(null)
  const synthRef = useRef<SpeechSynthesis | null>(null)
  const currentUtteranceRef = useRef<SpeechSynthesisUtterance | null>(null)

  useEffect(() => {
    // Speech Recognition ì´ˆê¸°í™”
    if (typeof window !== 'undefined') {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition
      if (SpeechRecognition) {
        const recognition = new SpeechRecognition()
        recognition.continuous = true
        recognition.interimResults = true
        recognition.lang = 'ko-KR'
        
        recognition.onstart = () => {
          setIsListening(true)
          setConnectionStatus('ìŒì„±ì„ ë“£ê³  ìˆìŠµë‹ˆë‹¤...')
        }
        
        recognition.onresult = (event: any) => {
          let interim = ''
          let final = ''
          
          for (let i = event.resultIndex; i < event.results.length; i++) {
            const transcript = event.results[i][0].transcript
            if (event.results[i].isFinal) {
              final += transcript
            } else {
              interim += transcript
            }
          }
          
          setInterimTranscript(interim)
          if (final) {
            setCurrentTranscript(final)
            handleUserSpeech(final)
          }
        }
        
        recognition.onerror = (event: any) => {
          console.error('ìŒì„± ì¸ì‹ ì˜¤ë¥˜:', event.error)
          setConnectionStatus(`ìŒì„± ì¸ì‹ ì˜¤ë¥˜: ${event.error}`)
        }
        
        recognition.onend = () => {
          setIsListening(false)
          if (isConnected && !isSpeaking) {
            // ìë™ìœ¼ë¡œ ë‹¤ì‹œ ë“£ê¸° ì‹œì‘
            setTimeout(() => {
              if (recognitionRef.current && isConnected) {
                recognitionRef.current.start()
              }
            }, 500)
          }
        }
        
        recognitionRef.current = recognition
      }
      
      // Speech Synthesis ì´ˆê¸°í™”
      synthRef.current = window.speechSynthesis
    }

    return () => {
      stopListening()
      stopSpeaking()
    }
  }, [isConnected, isSpeaking])

  const startInterview = useCallback(async () => {
    try {
      setConnectionStatus('ì¸í„°ë·°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...')
      setIsConnected(true)

      // ì²« ë²ˆì§¸ AI ì§ˆë¬¸ ìš”ì²­
      const response = await fetch('/api/interview/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ 
          sessionNumber,
          conversationHistory: []
        })
      })

      if (!response.ok) {
        throw new Error('ì¸í„°ë·° ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
      }

      const data = await response.json()
      
      const aiMessage: Conversation = {
        role: 'assistant',
        content: data.message,
        timestamp: new Date(),
        isSpeaking: true
      }
      
      setConversations([aiMessage])
      
      // AI ìŒì„±ìœ¼ë¡œ ì§ˆë¬¸ ì½ê¸°
      speakText(data.message)
      
      setConnectionStatus('AI ì§ˆë¬¸ì„ ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤...')

    } catch (error) {
      console.error('ì¸í„°ë·° ì‹œì‘ ì˜¤ë¥˜:', error)
      setConnectionStatus('ì¸í„°ë·° ì‹œì‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')
    }
  }, [sessionNumber])

  const handleUserSpeech = useCallback(async (transcript: string) => {
    if (!transcript.trim() || !isConnected) return

    setCurrentTranscript('')
    setInterimTranscript('')
    
    const userMessage: Conversation = {
      role: 'user',
      content: transcript.trim(),
      timestamp: new Date()
    }

    const newConversations = [...conversations, userMessage]
    setConversations(newConversations)
    
    setConnectionStatus('AIê°€ ì‘ë‹µì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...')
    stopListening()

    try {
      // AI ì‘ë‹µ ìš”ì²­
      const response = await fetch('/api/interview/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          sessionNumber,
          userMessage: userMessage.content,
          conversationHistory: conversations
        })
      })

      if (!response.ok) {
        throw new Error('AI ì‘ë‹µ ìš”ì²­ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
      }

      const data = await response.json()
      
      const aiMessage: Conversation = {
        role: 'assistant',
        content: data.message,
        timestamp: new Date(),
        isSpeaking: true
      }
      
      setConversations(prev => [...prev, aiMessage])

      // ëŒ€í™” ì €ì¥
      const lastAI = conversations[conversations.length - 1]
      if (lastAI && lastAI.role === 'assistant') {
        await onConversationSave(lastAI.content, userMessage.content)
      }

      // AI ì‘ë‹µì„ ìŒì„±ìœ¼ë¡œ ì½ê¸°
      speakText(data.message)

    } catch (error) {
      console.error('AI ì‘ë‹µ ì˜¤ë¥˜:', error)
      setConnectionStatus('AI ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')
      startListening() // ì˜¤ë¥˜ ì‹œ ë‹¤ì‹œ ë“£ê¸° ì‹œì‘
    }
  }, [conversations, isConnected, onConversationSave])

  const speakText = useCallback((text: string) => {
    if (!synthRef.current) return

    // ì´ì „ ìŒì„± ì¤‘ì§€
    stopSpeaking()

    const utterance = new SpeechSynthesisUtterance(text)
    utterance.lang = 'ko-KR'
    utterance.rate = 0.9
    utterance.pitch = 1.0
    
    utterance.onstart = () => {
      setIsSpeaking(true)
      setConnectionStatus('AIê°€ ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤...')
    }
    
    utterance.onend = () => {
      setIsSpeaking(false)
      currentUtteranceRef.current = null
      
      // AIê°€ ë§ì„ ë§ˆì¹˜ë©´ ì‚¬ìš©ì ìŒì„± ë“£ê¸° ì‹œì‘
      if (isConnected) {
        setTimeout(() => {
          startListening()
        }, 1000)
      }
    }
    
    utterance.onerror = (event) => {
      console.error('ìŒì„± í•©ì„± ì˜¤ë¥˜:', event)
      setIsSpeaking(false)
      if (isConnected) {
        startListening()
      }
    }
    
    currentUtteranceRef.current = utterance
    synthRef.current.speak(utterance)
  }, [isConnected])

  const startListening = useCallback(() => {
    if (recognitionRef.current && !isListening && !isSpeaking) {
      try {
        recognitionRef.current.start()
        setConnectionStatus('ë§ì”€í•´ ì£¼ì„¸ìš”...')
      } catch (error) {
        console.error('ìŒì„± ì¸ì‹ ì‹œì‘ ì˜¤ë¥˜:', error)
      }
    }
  }, [isListening, isSpeaking])

  const stopListening = useCallback(() => {
    if (recognitionRef.current && isListening) {
      recognitionRef.current.stop()
    }
  }, [isListening])

  const stopSpeaking = useCallback(() => {
    if (synthRef.current) {
      synthRef.current.cancel()
    }
    if (currentUtteranceRef.current) {
      currentUtteranceRef.current = null
    }
    setIsSpeaking(false)
  }, [])

  const disconnect = useCallback(() => {
    setIsConnected(false)
    stopListening()
    stopSpeaking()
    setConnectionStatus('ì¸í„°ë·°ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.')
  }, [stopListening, stopSpeaking])

  const skipToListening = useCallback(() => {
    stopSpeaking()
    startListening()
  }, [stopSpeaking, startListening])

  // Web Speech API ì§€ì› í™•ì¸
  const isWebSpeechSupported = typeof window !== 'undefined' && 
    (window.SpeechRecognition || window.webkitSpeechRecognition) && 
    window.speechSynthesis

  if (!isWebSpeechSupported) {
    return (
      <div className="bg-white rounded-lg shadow-lg p-6">
        <div className="text-center">
          <h3 className="text-xl font-semibold text-gray-800 mb-4">ìŒì„± ì¸í„°ë·° ì§€ì›ë˜ì§€ ì•ŠìŒ</h3>
          <p className="text-gray-600 mb-4">
            ì´ ë¸Œë¼ìš°ì €ëŠ” Web Speech APIë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
          </p>
          <p className="text-sm text-gray-500">
            Chrome, Edge, Safari ë“±ì˜ ìµœì‹  ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
          </p>
        </div>
      </div>
    )
  }

  return (
    <div className="bg-white rounded-lg shadow-lg p-6">
      <div className="mb-6">
        <h3 className="text-xl font-semibold text-gray-800 mb-2">ğŸ¤ ìŒì„± ì¸í„°ë·° (Web Speech API)</h3>
        <p className="text-gray-600">{connectionStatus}</p>
      </div>

      {/* ì—°ê²° ë²„íŠ¼ */}
      <div className="flex justify-center mb-6 space-x-4">
        {!isConnected ? (
          <button
            onClick={startInterview}
            className="px-6 py-3 bg-blue-600 text-white rounded-lg hover:bg-blue-700 transition font-medium"
          >
            ğŸ¤ ìŒì„± ì¸í„°ë·° ì‹œì‘
          </button>
        ) : (
          <>
            <button
              onClick={disconnect}
              className="px-6 py-3 bg-red-600 text-white rounded-lg hover:bg-red-700 transition font-medium"
            >
              ğŸ›‘ ì¸í„°ë·° ì¢…ë£Œ
            </button>
            {isSpeaking && (
              <button
                onClick={skipToListening}
                className="px-6 py-3 bg-yellow-600 text-white rounded-lg hover:bg-yellow-700 transition font-medium"
              >
                â­ï¸ AI ê±´ë„ˆë›°ê¸°
              </button>
            )}
            {!isListening && !isSpeaking && (
              <button
                onClick={startListening}
                className="px-6 py-3 bg-green-600 text-white rounded-lg hover:bg-green-700 transition font-medium"
              >
                ğŸ™ï¸ ë‹¤ì‹œ ë“£ê¸°
              </button>
            )}
          </>
        )}
      </div>

      {/* ìŒì„± ìƒíƒœ í‘œì‹œ */}
      {isConnected && (
        <div className="text-center mb-6">
          <div className="space-y-2">
            {isListening && (
              <div className="inline-flex items-center px-4 py-2 rounded-full bg-red-100 text-red-800">
                <div className="w-3 h-3 rounded-full mr-2 bg-red-500 animate-pulse" />
                ğŸ¤ ë“£ê³  ìˆìŠµë‹ˆë‹¤...
              </div>
            )}
            
            {isSpeaking && (
              <div className="inline-flex items-center px-4 py-2 rounded-full bg-blue-100 text-blue-800">
                <div className="w-3 h-3 rounded-full mr-2 bg-blue-500 animate-pulse" />
                ğŸ—£ï¸ AIê°€ ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤...
              </div>
            )}

            {!isListening && !isSpeaking && (
              <div className="inline-flex items-center px-4 py-2 rounded-full bg-gray-100 text-gray-600">
                <div className="w-3 h-3 rounded-full mr-2 bg-gray-400" />
                â¸ï¸ ëŒ€ê¸° ì¤‘...
              </div>
            )}
          </div>
        </div>
      )}

      {/* ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ í‘œì‹œ */}
      {(currentTranscript || interimTranscript) && (
        <div className="mb-4 p-4 bg-green-50 rounded-lg border-l-4 border-green-400">
          <p className="text-green-800 font-medium">ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹:</p>
          <p className="text-green-700 mt-1">
            {currentTranscript}
            <span className="text-gray-500 italic">{interimTranscript}</span>
          </p>
        </div>
      )}

      {/* ëŒ€í™” ê¸°ë¡ */}
      <div className="max-h-96 overflow-y-auto space-y-4">
        {conversations.map((conv, index) => (
          <div
            key={index}
            className={`p-4 rounded-lg ${
              conv.role === 'assistant' 
                ? 'bg-blue-50 border-l-4 border-blue-400' 
                : 'bg-green-50 border-l-4 border-green-400'
            }`}
          >
            <div className="flex justify-between items-start mb-2">
              <span className={`font-medium ${
                conv.role === 'assistant' ? 'text-blue-800' : 'text-green-800'
              }`}>
                {conv.role === 'assistant' ? 'ğŸ¤– AI ì¸í„°ë·°ì–´' : 'ğŸ‘¤ ì•„ë²„ë‹˜'}
                {conv.isSpeaking && (
                  <span className="ml-2 text-xs bg-blue-200 text-blue-700 px-2 py-1 rounded animate-pulse">
                    ë§í•˜ëŠ” ì¤‘
                  </span>
                )}
              </span>
              <span className="text-xs text-gray-500">
                {conv.timestamp.toLocaleTimeString()}
              </span>
            </div>
            <p className="text-gray-800 whitespace-pre-wrap">{conv.content}</p>
          </div>
        ))}

        {conversations.length === 0 && isConnected && (
          <div className="text-center text-gray-500 py-8">
            <p>ì¸í„°ë·°ê°€ ê³§ ì‹œì‘ë©ë‹ˆë‹¤...</p>
            <p className="text-sm mt-2">AIì˜ ì§ˆë¬¸ì„ ë“£ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.</p>
          </div>
        )}
      </div>

      {/* ì‚¬ìš© íŒ */}
      <div className="mt-4 p-3 bg-gray-50 rounded-lg text-sm text-gray-600">
        <h4 className="font-medium text-gray-800 mb-2">ğŸ’¡ ì‚¬ìš© íŒ:</h4>
        <ul className="space-y-1 text-xs">
          <li>â€¢ AIê°€ ì§ˆë¬¸ì„ ë§í•˜ë©´ ìë™ìœ¼ë¡œ ìŒì„± ì¸ì‹ì´ ì‹œì‘ë©ë‹ˆë‹¤</li>
          <li>â€¢ ë‹µë³€ì„ ë§ˆì¹˜ë©´ ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš” (ìë™ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸)</li>
          <li>â€¢ AIê°€ ë§í•˜ëŠ” ì¤‘ì— "AI ê±´ë„ˆë›°ê¸°"ë¡œ ë°”ë¡œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤</li>
          <li>â€¢ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”</li>
          <li>â€¢ ì¡°ìš©í•œ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ì‹œë©´ ë” ì •í™•í•©ë‹ˆë‹¤</li>
        </ul>
      </div>
    </div>
  )
}