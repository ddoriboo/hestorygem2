'use client'

import { useState, useEffect, useRef } from 'react'
import { getSessionPrompt } from '@/lib/session-prompts'

interface VoiceInterviewProps {
  sessionId: string
  sessionNumber: number
  onConversationSave: (question: string, answer: string) => Promise<void>
}

interface Conversation {
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
}

export default function VoiceInterview({ sessionId, sessionNumber, onConversationSave }: VoiceInterviewProps) {
  const [isConnected, setIsConnected] = useState(false)
  const [isRecording, setIsRecording] = useState(false)
  const [conversations, setConversations] = useState<Conversation[]>([])
  const [connectionStatus, setConnectionStatus] = useState<string>('ì—°ê²° ì¤€ë¹„ ì¤‘...')
  const [currentQuestion, setCurrentQuestion] = useState('')
  const [userResponse, setUserResponse] = useState('')

  const wsRef = useRef<WebSocket | null>(null)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const conversationRef = useRef<Conversation[]>([])

  useEffect(() => {
    conversationRef.current = conversations
  }, [conversations])

  const connectToRealtime = async () => {
    try {
      setConnectionStatus('ìŒì„± ì¸í„°ë·° ì¤€ë¹„ ì¤‘...')
      
      // Realtime API ì„¤ì • ìš”ì²­
      const response = await fetch('/api/interview/realtime', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ sessionNumber })
      })

      if (!response.ok) {
        throw new Error('ì¸í„°ë·° ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
      }

      const { apiKey, sessionPrompt } = await response.json()

      // WebSocket ì—°ê²° (ë¸Œë¼ìš°ì €ì—ì„œëŠ” headersë¥¼ URL íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬í•  ìˆ˜ ì—†ìŒ)
      // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì„œë²„ì—ì„œ í”„ë¡ì‹œ ì—­í• ì„ í•´ì•¼ í•¨
      const ws = new WebSocket(
        `wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01&authorization=${encodeURIComponent(apiKey)}`
      )

      wsRef.current = ws

      ws.onopen = () => {
        setIsConnected(true)
        setConnectionStatus('ìŒì„± ì¸í„°ë·° ì¤€ë¹„ ì™„ë£Œ')
        
        // ì„¸ì…˜ ì„¤ì •
        const sessionUpdate = {
          type: 'session.update',
          session: {
            instructions: sessionPrompt,
            voice: 'alloy',
            input_audio_format: 'pcm16',
            output_audio_format: 'pcm16',
            turn_detection: {
              type: 'server_vad',
              threshold: 0.5,
              prefix_padding_ms: 300,
              silence_duration_ms: 500
            }
          }
        }
        
        ws.send(JSON.stringify(sessionUpdate))
        
        // ì´ˆê¸° ëŒ€í™” ì‹œì‘
        ws.send(JSON.stringify({ type: 'response.create' }))
      }

      ws.onmessage = handleServerEvent
      
      ws.onerror = (error) => {
        console.error('WebSocket ì˜¤ë¥˜:', error)
        setConnectionStatus('ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')
      }

      ws.onclose = () => {
        setIsConnected(false)
        setConnectionStatus('ì—°ê²°ì´ í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤.')
      }

      // ë§ˆì´í¬ ì„¤ì •
      await setupAudio()

    } catch (error) {
      console.error('ì—°ê²° ì˜¤ë¥˜:', error)
      setConnectionStatus(`ì—°ê²° ì‹¤íŒ¨: ${error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`)
    }
  }

  const setupAudio = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 24000,
          channelCount: 1,
          sampleSize: 16
        }
      })

      const audioContext = new AudioContext({ sampleRate: 24000 })
      audioContextRef.current = audioContext

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=pcm'
      })
      mediaRecorderRef.current = mediaRecorder

      mediaRecorder.ondataavailable = async (event) => {
        if (event.data.size > 0 && wsRef.current?.readyState === WebSocket.OPEN) {
          const arrayBuffer = await event.data.arrayBuffer()
          const audioData = new Int16Array(arrayBuffer)
          const base64Audio = btoa(String.fromCharCode(...new Uint8Array(audioData.buffer)))
          
          wsRef.current.send(JSON.stringify({
            type: 'input_audio_buffer.append',
            audio: base64Audio
          }))
        }
      }

      mediaRecorder.onstart = () => setIsRecording(true)
      mediaRecorder.onstop = () => {
        setIsRecording(false)
        if (wsRef.current?.readyState === WebSocket.OPEN) {
          wsRef.current.send(JSON.stringify({
            type: 'input_audio_buffer.commit'
          }))
          wsRef.current.send(JSON.stringify({
            type: 'response.create'
          }))
        }
      }

    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ ì„¤ì • ì˜¤ë¥˜:', error)
      setConnectionStatus('ë§ˆì´í¬ ì•¡ì„¸ìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.')
    }
  }

  const handleServerEvent = (event: MessageEvent) => {
    try {
      const data = JSON.parse(event.data)
      console.log('ì„œë²„ ì´ë²¤íŠ¸:', data.type, data)
      
      switch (data.type) {
        case 'session.created':
          console.log('ì„¸ì…˜ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.')
          break
          
        case 'response.audio_transcript.delta':
          // AI ì‘ë‹µ í…ìŠ¤íŠ¸ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
          setCurrentQuestion(prev => prev + data.delta)
          break
          
        case 'response.audio_transcript.done':
          // AI ì‘ë‹µ ì™„ë£Œ
          const aiMessage: Conversation = {
            role: 'assistant',
            content: data.transcript,
            timestamp: new Date()
          }
          setConversations(prev => [...prev, aiMessage])
          setCurrentQuestion('')
          break
          
        case 'response.audio.delta':
          // ì˜¤ë””ì˜¤ ë°ì´í„° ìˆ˜ì‹  ë° ì¬ìƒ
          playAudioChunk(data.delta)
          break
          
        case 'input_audio_buffer.speech_started':
          setUserResponse('ë§ì”€í•˜ê³  ê³„ì‹­ë‹ˆë‹¤...')
          if (mediaRecorderRef.current?.state === 'inactive') {
            mediaRecorderRef.current.start(250) // 250ms ê°„ê²©ìœ¼ë¡œ ë°ì´í„° ì „ì†¡
          }
          break
          
        case 'input_audio_buffer.speech_stopped':
          setUserResponse('')
          if (mediaRecorderRef.current?.state === 'recording') {
            mediaRecorderRef.current.stop()
          }
          break
          
        case 'conversation.item.input_audio_transcription.completed':
          // ì‚¬ìš©ì ì‘ë‹µ í…ìŠ¤íŠ¸
          const userMessage: Conversation = {
            role: 'user',
            content: data.transcript,
            timestamp: new Date()
          }
          setConversations(prev => [...prev, userMessage])
          
          // ëŒ€í™” ì €ì¥
          const currentConversations = [...conversationRef.current, userMessage]
          if (currentConversations.length >= 2) {
            const lastAI = currentConversations.findLast(c => c.role === 'assistant')
            const lastUser = currentConversations.findLast(c => c.role === 'user')
            
            if (lastAI && lastUser) {
              onConversationSave(lastAI.content, lastUser.content)
            }
          }
          break
      }
    } catch (error) {
      console.error('ì„œë²„ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜:', error)
    }
  }

  const playAudioChunk = (audioData: string) => {
    try {
      if (!audioContextRef.current) return
      
      const binaryString = atob(audioData)
      const audioBuffer = new ArrayBuffer(binaryString.length)
      const audioView = new Uint8Array(audioBuffer)
      
      for (let i = 0; i < binaryString.length; i++) {
        audioView[i] = binaryString.charCodeAt(i)
      }
      
      audioContextRef.current.decodeAudioData(audioBuffer)
        .then(decodedBuffer => {
          const source = audioContextRef.current!.createBufferSource()
          source.buffer = decodedBuffer
          source.connect(audioContextRef.current!.destination)
          source.start()
        })
        .catch(err => console.error('ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜:', err))
    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜:', error)
    }
  }

  const disconnect = () => {
    if (wsRef.current) {
      wsRef.current.close()
      wsRef.current = null
    }
    if (mediaRecorderRef.current) {
      mediaRecorderRef.current.stop()
      mediaRecorderRef.current = null
    }
    if (audioContextRef.current) {
      audioContextRef.current.close()
      audioContextRef.current = null
    }
    setIsConnected(false)
    setIsRecording(false)
    setConnectionStatus('ì—°ê²° í•´ì œë¨')
  }

  const startManualRecording = () => {
    if (mediaRecorderRef.current?.state === 'inactive') {
      mediaRecorderRef.current.start(250)
      setIsRecording(true)
    }
  }

  const stopManualRecording = () => {
    if (mediaRecorderRef.current?.state === 'recording') {
      mediaRecorderRef.current.stop()
      setIsRecording(false)
    }
  }

  return (
    <div className="bg-white rounded-lg shadow-lg p-6">
      <div className="mb-6">
        <h3 className="text-xl font-semibold text-gray-800 mb-2">ìŒì„± ì¸í„°ë·°</h3>
        <p className="text-gray-600">{connectionStatus}</p>
      </div>

      {/* ì—°ê²° ë²„íŠ¼ */}
      <div className="flex justify-center mb-6">
        {!isConnected ? (
          <button
            onClick={connectToRealtime}
            className="px-6 py-3 bg-blue-600 text-white rounded-lg hover:bg-blue-700 transition"
          >
            ìŒì„± ì¸í„°ë·° ì‹œì‘
          </button>
        ) : (
          <button
            onClick={disconnect}
            className="px-6 py-3 bg-red-600 text-white rounded-lg hover:bg-red-700 transition"
          >
            ì¸í„°ë·° ì¢…ë£Œ
          </button>
        )}
      </div>

      {/* ìŒì„± ìƒíƒœ í‘œì‹œ */}
      {isConnected && (
        <div className="text-center mb-6">
          <div className={`inline-flex items-center px-4 py-2 rounded-full ${
            isRecording ? 'bg-red-100 text-red-800' : 'bg-green-100 text-green-800'
          }`}>
            <div className={`w-3 h-3 rounded-full mr-2 ${
              isRecording ? 'bg-red-500 animate-pulse' : 'bg-green-500'
            }`} />
            {isRecording ? 'ë§ì”€í•˜ê³  ê³„ì‹­ë‹ˆë‹¤...' : 'ì¸í„°ë·°ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤'}
          </div>
        </div>
      )}

      {/* í˜„ì¬ ì§ˆë¬¸ í‘œì‹œ */}
      {currentQuestion && (
        <div className="mb-4 p-4 bg-blue-50 rounded-lg">
          <p className="text-blue-800 font-medium">AI:</p>
          <p className="text-blue-700 mt-1">{currentQuestion}</p>
        </div>
      )}

      {/* ì‚¬ìš©ì ì‘ë‹µ ìƒíƒœ */}
      {userResponse && (
        <div className="mb-4 p-4 bg-gray-50 rounded-lg">
          <p className="text-gray-600 italic">{userResponse}</p>
        </div>
      )}

      {/* ìˆ˜ë™ ë…¹ìŒ ë²„íŠ¼ (VADê°€ ì‘ë™í•˜ì§€ ì•Šì„ ê²½ìš°) */}
      {isConnected && (
        <div className="flex justify-center space-x-4 mb-6">
          {!isRecording ? (
            <button
              onClick={startManualRecording}
              className="px-6 py-3 bg-orange-600 text-white rounded-lg hover:bg-orange-700 transition"
            >
              ğŸ¤ ìˆ˜ë™ ë…¹ìŒ ì‹œì‘
            </button>
          ) : (
            <button
              onClick={stopManualRecording}
              className="px-6 py-3 bg-red-600 text-white rounded-lg hover:bg-red-700 transition"
            >
              ğŸ›‘ ë…¹ìŒ ì •ì§€
            </button>
          )}
        </div>
      )}

      {/* ëŒ€í™” ê¸°ë¡ */}
      <div className="max-h-96 overflow-y-auto space-y-4">
        {conversations.map((conv, index) => (
          <div
            key={index}
            className={`p-4 rounded-lg ${
              conv.role === 'assistant' 
                ? 'bg-blue-50 border-l-4 border-blue-400' 
                : 'bg-green-50 border-l-4 border-green-400'
            }`}
          >
            <div className="flex justify-between items-start mb-2">
              <span className={`font-medium ${
                conv.role === 'assistant' ? 'text-blue-800' : 'text-green-800'
              }`}>
                {conv.role === 'assistant' ? 'AI' : 'ì•„ë²„ë‹˜'}
              </span>
              <span className="text-xs text-gray-500">
                {conv.timestamp.toLocaleTimeString()}
              </span>
            </div>
            <p className="text-gray-800">{conv.content}</p>
          </div>
        ))}
      </div>
    </div>
  )
}