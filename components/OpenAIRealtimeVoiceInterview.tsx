'use client'

import { useState, useEffect, useRef, useCallback } from 'react'

interface OpenAIRealtimeVoiceInterviewProps {
  sessionNumber: number
  onConversationSave: (question: string, answer: string) => Promise<void>
}

interface Conversation {
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
  audioComplete?: boolean
}

export default function OpenAIRealtimeVoiceInterview({ sessionNumber, onConversationSave }: OpenAIRealtimeVoiceInterviewProps) {
  const [isConnected, setIsConnected] = useState(false)
  const [isRecording, setIsRecording] = useState(false)
  const [conversations, setConversations] = useState<Conversation[]>([])
  const [connectionStatus, setConnectionStatus] = useState('ì—°ê²° ì¤€ë¹„ ì¤‘...')
  const [currentTranscript, setCurrentTranscript] = useState('')
  const [isAISpeaking, setIsAISpeaking] = useState(false)

  // WebRTC refs
  const peerConnectionRef = useRef<RTCPeerConnection | null>(null)
  const dataChannelRef = useRef<RTCDataChannel | null>(null)
  const audioStreamRef = useRef<MediaStream | null>(null)

  useEffect(() => {
    return () => {
      disconnect()
    }
  }, [])

  const connectToRealtime = useCallback(async () => {
    try {
      setConnectionStatus('ì„¸ì…˜ í† í°ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...')

      // OpenAI Realtime API ì„¸ì…˜ í† í° ìš”ì²­
      const tokenResponse = await fetch('/api/interview/realtime-token', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ sessionNumber })
      })

      if (!tokenResponse.ok) {
        throw new Error('ì„¸ì…˜ í† í° ìš”ì²­ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
      }

      const { apiKey, sessionPrompt, model, voice } = await tokenResponse.json()
      console.log('API ì„¤ì • ë°›ìŒ:', { model, voice })

      setConnectionStatus('ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ì¤‘...')

      // ë§ˆì´í¬ ì„¤ì • (ëª¨ë°”ì¼ í˜¸í™˜ì„± ê°œì„ )
      try {
        // ëª¨ë°”ì¼ì—ì„œë„ í˜¸í™˜ë˜ëŠ” ê°„ë‹¨í•œ constraints ì‚¬ìš©
        const audioConstraints = {
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            // ëª¨ë°”ì¼ì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ì„¤ì •ë“¤ ì œê±°
            ...(typeof window !== 'undefined' && !navigator.userAgent.match(/iPhone|iPad|iPod|Android/i) && {
              sampleRate: 24000,
              channelCount: 1
            })
          }
        }

        const stream = await navigator.mediaDevices.getUserMedia(audioConstraints)
        audioStreamRef.current = stream
        console.log('ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ì„¤ì • ì™„ë£Œ')
      } catch (error: any) {
        console.error('ë§ˆì´í¬ ì•¡ì„¸ìŠ¤ ì‹¤íŒ¨:', error)
        
        // ë” êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€
        if (error.name === 'NotAllowedError') {
          setConnectionStatus('ë§ˆì´í¬ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.')
        } else if (error.name === 'NotFoundError') {
          setConnectionStatus('ë§ˆì´í¬ê°€ ê°ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
        } else if (error.name === 'NotSupportedError') {
          setConnectionStatus('ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ê¸°ëŠ¥ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
        } else {
          setConnectionStatus(`ë§ˆì´í¬ ì˜¤ë¥˜: ${error.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`)
        }
        return
      }

      setConnectionStatus('OpenAI Realtime API ì—°ê²° ì¤‘...')

      // WebRTC ì„¤ì •
      await setupWebRTC(apiKey, sessionPrompt, model, voice)

    } catch (error) {
      console.error('Realtime ì—°ê²° ì˜¤ë¥˜:', error)
      setConnectionStatus(`ì—°ê²° ì‹¤íŒ¨: ${error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`)
    }
  }, [sessionNumber])

  const setupWebRTC = useCallback(async (apiKey: string, sessionPrompt: string, model: string, voice: string) => {
    try {
      // RTCPeerConnection ìƒì„±
      const peerConnection = new RTCPeerConnection()
      peerConnectionRef.current = peerConnection

      // ì˜¤ë””ì˜¤ ì¶œë ¥ ì„¤ì •
      const audioElement = document.createElement('audio')
      audioElement.autoplay = true
      
      peerConnection.ontrack = (event) => {
        console.log('ì›ê²© ì˜¤ë””ì˜¤ íŠ¸ë™ ìˆ˜ì‹ :', event.streams[0])
        audioElement.srcObject = event.streams[0]
      }

      // ë¡œì»¬ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì¶”ê°€
      if (audioStreamRef.current) {
        const audioTrack = audioStreamRef.current.getTracks()[0]
        peerConnection.addTrack(audioTrack, audioStreamRef.current)
      }

      // ë°ì´í„° ì±„ë„ ì„¤ì •
      const dataChannel = peerConnection.createDataChannel('oai-events')
      dataChannelRef.current = dataChannel

      dataChannel.onopen = () => {
        console.log('ë°ì´í„° ì±„ë„ ì—°ê²°ë¨')
        setIsConnected(true)
        setConnectionStatus('ì„¸ì…˜ ì„¤ì • ì¤‘...')

        // 1. ì„¸ì…˜ ì„¤ì •: ìŒì„± ì „ì‚¬ í™œì„±í™” ë° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        const sessionUpdateMessage = {
          type: 'session.update',
          session: {
            instructions: sessionPrompt,
            voice: voice,
            input_audio_format: 'pcm16',
            output_audio_format: 'pcm16',
            input_audio_transcription: {
              model: 'whisper-1'
            },
            turn_detection: {
              type: 'server_vad',
              threshold: 0.5,
              prefix_padding_ms: 300,
              silence_duration_ms: 200
            },
            tools: [],
            tool_choice: 'auto',
            temperature: 0.8,
            max_response_output_tokens: 4096
          }
        }

        console.log('ì„¸ì…˜ ì„¤ì • ë©”ì‹œì§€ ì „ì†¡:', sessionUpdateMessage)
        dataChannel.send(JSON.stringify(sessionUpdateMessage))

        // 2. ì ì‹œ ëŒ€ê¸° í›„ ì²« ë²ˆì§¸ ì‘ë‹µ ìš”ì²­
        setTimeout(() => {
          setConnectionStatus('ìŒì„± ì¸í„°ë·° ì¤€ë¹„ ì™„ë£Œ')
          setIsRecording(true)
          console.log('ì²« ë²ˆì§¸ ì‘ë‹µ ìš”ì²­ ì „ì†¡')
          sendMessage({ type: 'response.create' })
        }, 1500)
      }

      dataChannel.onmessage = handleRealtimeEvent
      dataChannel.onerror = (error) => {
        console.error('ë°ì´í„° ì±„ë„ ì˜¤ë¥˜:', error)
        setConnectionStatus('ë°ì´í„° ì±„ë„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')
      }

      dataChannel.onclose = () => {
        console.log('ë°ì´í„° ì±„ë„ ì—°ê²° í•´ì œ')
        setIsConnected(false)
        setConnectionStatus('ì—°ê²° í•´ì œë¨')
      }

      // ICE ìƒíƒœ ë³€í™” ëª¨ë‹ˆí„°ë§
      peerConnection.oniceconnectionstatechange = () => {
        console.log('ICE ì—°ê²° ìƒíƒœ:', peerConnection.iceConnectionState)
        if (peerConnection.iceConnectionState === 'failed') {
          setConnectionStatus('ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
        }
      }

      // SDP Offer ìƒì„±
      const offer = await peerConnection.createOffer()
      await peerConnection.setLocalDescription(offer)

      // OpenAI Realtime APIì— SDP offer ì „ì†¡
      const realtimeResponse = await fetch('https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${apiKey}`,
          'Content-Type': 'application/sdp'
        },
        body: offer.sdp
      })

      if (!realtimeResponse.ok) {
        throw new Error(`Realtime API ì—°ê²° ì‹¤íŒ¨: ${realtimeResponse.status}`)
      }

      const answerSdp = await realtimeResponse.text()
      await peerConnection.setRemoteDescription({
        type: 'answer',
        sdp: answerSdp
      })

      console.log('OpenAI Realtime API WebRTC ì—°ê²° ì™„ë£Œ')

    } catch (error) {
      console.error('WebRTC ì„¤ì • ì˜¤ë¥˜:', error)
      setConnectionStatus(`WebRTC ì„¤ì • ì‹¤íŒ¨: ${error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`)
    }
  }, [])

  const addUserMessage = useCallback(async (transcript: string) => {
    const userMessage: Conversation = {
      role: 'user',
      content: transcript,
      timestamp: new Date()
    }
    
    setConversations(prev => {
      // ì¤‘ë³µ ë©”ì‹œì§€ ë°©ì§€
      const lastMessage = prev[prev.length - 1]
      if (lastMessage && lastMessage.role === 'user' && lastMessage.content === transcript) {
        return prev
      }
      
      const newConversations = [...prev, userMessage]
      
      // ëŒ€í™” ì €ì¥ (ê°€ì¥ ìµœê·¼ AI ì§ˆë¬¸ê³¼ ì‚¬ìš©ì ë‹µë³€ ìŒìœ¼ë¡œ ì €ì¥)
      const lastAssistant = newConversations
        .slice()
        .reverse()
        .find(conv => conv.role === 'assistant')
      
      if (lastAssistant) {
        console.log('ğŸ’¾ ëŒ€í™” ì €ì¥ ì‹œë„:', lastAssistant.content, userMessage.content)
        // ë¹„ë™ê¸°ë¡œ ì €ì¥í•˜ë˜ ì—ëŸ¬ëŠ” ë¬´ì‹œ (UI ì°¨ë‹¨ ë°©ì§€)
        onConversationSave(lastAssistant.content, userMessage.content)
          .then(() => console.log('âœ… ëŒ€í™” ì €ì¥ ì„±ê³µ'))
          .catch((error) => {
            console.error('âŒ ëŒ€í™” ì €ì¥ ì‹¤íŒ¨:', error)
            // ì €ì¥ ì‹¤íŒ¨í•´ë„ ëŒ€í™”ëŠ” ê³„ì† ì§„í–‰ (ì‚¬ìš©ì ê²½í—˜ ìš°ì„ )
          })
      }
      
      return newConversations
    })
  }, [onConversationSave])

  const handleRealtimeEvent = useCallback((event: MessageEvent) => {
    try {
      const data = JSON.parse(event.data)
      console.log('ğŸ”„ Realtime ì´ë²¤íŠ¸:', data.type, data)

      switch (data.type) {
        case 'session.created':
          console.log('ì„¸ì…˜ ìƒì„±ë¨:', data.session)
          break

        case 'session.updated':
          console.log('ì„¸ì…˜ ì—…ë°ì´íŠ¸ë¨')
          break

        case 'response.created':
          console.log('ì‘ë‹µ ìƒì„± ì‹œì‘')
          setIsAISpeaking(true)
          break

        case 'response.audio_transcript.delta':
          // AI ì‘ë‹µ í…ìŠ¤íŠ¸ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
          setCurrentTranscript(prev => prev + (data.delta || ''))
          break

        case 'response.audio_transcript.done':
          // AI ì‘ë‹µ ì™„ë£Œ (ë©”ì‹œì§€ ì¶”ê°€ëŠ” ì—¬ê¸°ì„œë§Œ ì²˜ë¦¬)
          if (data.transcript && data.transcript.trim()) {
            const assistantMessage: Conversation = {
              role: 'assistant',
              content: data.transcript.trim(),
              timestamp: new Date(),
              audioComplete: true
            }
            setConversations(prev => [...prev, assistantMessage])
          }
          setCurrentTranscript('')
          setIsAISpeaking(false)
          break

        case 'conversation.item.input_audio_transcription.completed':
          // ì‚¬ìš©ì ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜ ì™„ë£Œ
          console.log('ğŸ™ï¸ ì‚¬ìš©ì ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜ ì™„ë£Œ:', data)
          if (data.transcript && data.transcript.trim()) {
            addUserMessage(data.transcript.trim())
          }
          break

        case 'conversation.item.created':
          // ëŒ€í™” ì•„ì´í…œ ìƒì„±ë¨ (ì‚¬ìš©ì ë©”ì‹œì§€ í¬í•¨)
          console.log('ğŸ’¬ ëŒ€í™” ì•„ì´í…œ ìƒì„±ë¨:', data)
          if (data.item?.type === 'message' && data.item?.role === 'user') {
            const content = data.item.content
            if (content && content.length > 0) {
              const transcript = content
                .map((c: any) => c.text || c.transcript || c.audio?.transcript || '')
                .filter(Boolean)
                .join(' ')
              if (transcript && transcript.trim()) {
                addUserMessage(transcript.trim())
              }
            }
          }
          break

        case 'item.input_audio_transcription.completed':
          // ë‹¤ë¥¸ í˜•íƒœì˜ ì‚¬ìš©ì ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜ ì™„ë£Œ ì´ë²¤íŠ¸
          console.log('ğŸ¤ ìŒì„± ë³€í™˜ ì™„ë£Œ (ë‹¤ë¥¸ í˜•íƒœ):', data)
          if (data.transcript && data.transcript.trim()) {
            addUserMessage(data.transcript.trim())
          }
          break

        case 'input_audio_buffer.speech_started':
          console.log('ì‚¬ìš©ì ìŒì„± ê°ì§€ ì‹œì‘')
          setIsRecording(true)
          break

        case 'input_audio_buffer.speech_stopped':
          console.log('ì‚¬ìš©ì ìŒì„± ê°ì§€ ì¤‘ì§€')
          setIsRecording(false)
          break

        case 'response.done':
          console.log('ì‘ë‹µ ì™„ë£Œ:', data.response)
          setIsAISpeaking(false)
          // ë©”ì‹œì§€ ì¶”ê°€ëŠ” response.audio_transcript.doneì—ì„œë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œê±°
          break

        case 'error':
          console.error('Realtime API ì˜¤ë¥˜:', data)
          setConnectionStatus(`ì˜¤ë¥˜: ${data.error?.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`)
          break

        default:
          console.log('ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì´ë²¤íŠ¸:', data.type)
      }
    } catch (error) {
      console.error('ì´ë²¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜:', error)
    }
  }, [addUserMessage])

  const sendMessage = useCallback((message: any) => {
    if (dataChannelRef.current?.readyState === 'open') {
      dataChannelRef.current.send(JSON.stringify(message))
    }
  }, [])

  const disconnect = useCallback(() => {
    console.log('ì—°ê²° í•´ì œ ì¤‘...')

    // ë°ì´í„° ì±„ë„ í•´ì œ
    if (dataChannelRef.current) {
      dataChannelRef.current.close()
      dataChannelRef.current = null
    }

    // í”¼ì–´ ì—°ê²° í•´ì œ
    if (peerConnectionRef.current) {
      peerConnectionRef.current.close()
      peerConnectionRef.current = null
    }

    // ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ í•´ì œ
    if (audioStreamRef.current) {
      audioStreamRef.current.getTracks().forEach(track => track.stop())
      audioStreamRef.current = null
    }

    setIsConnected(false)
    setIsRecording(false)
    setIsAISpeaking(false)
    setConnectionStatus('ì—°ê²° í•´ì œë¨')
  }, [])

  const interruptAI = useCallback(() => {
    if (isAISpeaking) {
      sendMessage({ type: 'response.cancel' })
      setIsAISpeaking(false)
    }
  }, [isAISpeaking, sendMessage])

  // ëª¨ë°”ì¼ ë¸Œë¼ìš°ì € ì²´í¬
  const isMobile = typeof window !== 'undefined' && navigator.userAgent.match(/iPhone|iPad|iPod|Android/i)
  const isIOS = typeof window !== 'undefined' && navigator.userAgent.match(/iPhone|iPad|iPod/i)

  return (
    <div className="bg-white rounded-lg shadow-lg p-4 sm:p-6">
      <div className="mb-4 sm:mb-6">
        <h3 className="text-lg sm:text-xl font-semibold text-gray-800 mb-2">ğŸ¤ OpenAI Realtime ìŒì„± ì¸í„°ë·°</h3>
        <p className="text-sm sm:text-base text-gray-600">{connectionStatus}</p>
        {isMobile && (
          <p className="text-xs sm:text-sm text-amber-600 mt-2">
            ğŸ“± ëª¨ë°”ì¼ í™˜ê²½ì…ë‹ˆë‹¤. {isIOS ? 'Safari' : 'Chrome'} ë¸Œë¼ìš°ì € ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
          </p>
        )}
      </div>

      {/* ì—°ê²° ë²„íŠ¼ */}
      <div className="flex justify-center mb-4 sm:mb-6 space-x-2 sm:space-x-4">
        {!isConnected ? (
          <button
            onClick={connectToRealtime}
            className="px-4 py-2 sm:px-6 sm:py-3 bg-blue-600 text-white text-sm sm:text-base rounded-lg hover:bg-blue-700 transition font-medium"
          >
            ğŸ¤ OpenAI ìŒì„± ì¸í„°ë·° ì‹œì‘
          </button>
        ) : (
          <>
            <button
              onClick={disconnect}
              className="px-4 py-2 sm:px-6 sm:py-3 bg-red-600 text-white text-sm sm:text-base rounded-lg hover:bg-red-700 transition font-medium"
            >
              ğŸ›‘ ì¸í„°ë·° ì¢…ë£Œ
            </button>
            {isAISpeaking && (
              <button
                onClick={interruptAI}
                className="px-4 py-2 sm:px-6 sm:py-3 bg-yellow-600 text-white text-sm sm:text-base rounded-lg hover:bg-yellow-700 transition font-medium"
              >
                â¸ï¸ AI ì¤‘ë‹¨
              </button>
            )}
          </>
        )}
      </div>

      {/* ìŒì„± ìƒíƒœ í‘œì‹œ */}
      {isConnected && (
        <div className="text-center mb-6">
          <div className="space-y-2">
            <div className={`inline-flex items-center px-4 py-2 rounded-full ${
              isRecording ? 'bg-red-100 text-red-800' : 'bg-gray-100 text-gray-600'
            }`}>
              <div className={`w-3 h-3 rounded-full mr-2 ${
                isRecording ? 'bg-red-500 animate-pulse' : 'bg-gray-400'
              }`} />
              {isRecording ? 'ğŸ¤ ë§ì”€í•˜ê³  ê³„ì‹­ë‹ˆë‹¤...' : 'ğŸ§ AI ì¸í„°ë·°ì–´ê°€ ë“£ê³  ìˆìŠµë‹ˆë‹¤'}
            </div>
            
            {isAISpeaking && (
              <div className="inline-flex items-center px-4 py-2 rounded-full bg-blue-100 text-blue-800">
                <div className="w-3 h-3 rounded-full mr-2 bg-blue-500 animate-pulse" />
                ğŸ—£ï¸ AIê°€ ì‘ë‹µí•˜ê³  ìˆìŠµë‹ˆë‹¤...
              </div>
            )}
          </div>
        </div>
      )}

      {/* í˜„ì¬ AI ì‘ë‹µ í‘œì‹œ */}
      {currentTranscript && (
        <div className="mb-4 p-4 bg-blue-50 rounded-lg border-l-4 border-blue-400">
          <p className="text-blue-800 font-medium">AI (ì‹¤ì‹œê°„):</p>
          <p className="text-blue-700 mt-1">{currentTranscript}</p>
        </div>
      )}

      {/* ëŒ€í™” ê¸°ë¡ */}
      <div className="max-h-96 overflow-y-auto space-y-4">
        {conversations.map((conv, index) => (
          <div
            key={index}
            className={`p-4 rounded-lg ${
              conv.role === 'assistant' 
                ? 'bg-blue-50 border-l-4 border-blue-400' 
                : 'bg-green-50 border-l-4 border-green-400'
            }`}
          >
            <div className="flex justify-between items-start mb-2">
              <span className={`font-medium ${
                conv.role === 'assistant' ? 'text-blue-800' : 'text-green-800'
              }`}>
                {conv.role === 'assistant' ? 'ğŸ¤– AI ì¸í„°ë·°ì–´' : 'ğŸ‘¤ ì•„ë²„ë‹˜'}
                {conv.audioComplete && conv.role === 'assistant' && (
                  <span className="ml-2 text-xs bg-blue-200 text-blue-700 px-2 py-1 rounded">ìŒì„± ì™„ë£Œ</span>
                )}
              </span>
              <span className="text-xs text-gray-500">
                {conv.timestamp.toLocaleTimeString()}
              </span>
            </div>
            <p className="text-gray-800 whitespace-pre-wrap">{conv.content}</p>
          </div>
        ))}

        {conversations.length === 0 && isConnected && (
          <div className="text-center text-gray-500 py-8">
            <p>OpenAI Realtime ì¸í„°ë·°ê°€ ê³§ ì‹œì‘ë©ë‹ˆë‹¤...</p>
            <p className="text-sm mt-2">ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•˜ê³  AIì˜ ì§ˆë¬¸ì„ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.</p>
          </div>
        )}
      </div>

      {/* ì‚¬ìš© íŒ */}
      {isConnected && (
        <div className="mt-4 p-3 bg-gray-50 rounded-lg text-sm text-gray-600">
          <h4 className="font-medium text-gray-800 mb-2">ğŸ’¡ ì‚¬ìš© íŒ:</h4>
          <ul className="space-y-1 text-xs">
            <li>â€¢ OpenAI Realtime APIë¡œ ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤</li>
            <li>â€¢ ìŒì„± ê°ì§€ê°€ ìë™ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤ (VAD)</li>
            <li>â€¢ AIê°€ ë§í•˜ëŠ” ì¤‘ì—ë„ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤</li>
            <li>â€¢ ë‹µë³€ì´ ê¸¸ì–´ë„ ê´œì°®ìŠµë‹ˆë‹¤ - í¸ì•ˆí•˜ê²Œ ì´ì•¼ê¸°í•´ì£¼ì„¸ìš”</li>
            <li>â€¢ WebRTC ê¸°ìˆ ë¡œ ë‚®ì€ ì§€ì—°ì‹œê°„ì„ ì œê³µí•©ë‹ˆë‹¤</li>
          </ul>
        </div>
      )}
    </div>
  )
}