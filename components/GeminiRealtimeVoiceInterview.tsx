'use client'

import { useState, useEffect, useRef, useCallback } from 'react'
import { float32ToInt16, int16ToFloat32 } from '@/lib/audio-utils'

interface GeminiRealtimeVoiceInterviewProps {
  sessionNumber: number
  onConversationSave: (question: string, answer: string) => Promise<void>
}

interface Conversation {
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
  audioComplete?: boolean
}

export default function GeminiRealtimeVoiceInterview({ 
  sessionNumber, 
  onConversationSave 
}: GeminiRealtimeVoiceInterviewProps) {
  const [isConnected, setIsConnected] = useState(false)
  const [isRecording, setIsRecording] = useState(false)
  const [conversations, setConversations] = useState<Conversation[]>([])
  const [connectionStatus, setConnectionStatus] = useState('ì—°ê²° ì¤€ë¹„ ì¤‘...')
  const [isAISpeaking, setIsAISpeaking] = useState(false)

  // Session and Audio refs
  const sessionIdRef = useRef<string | null>(null)
  const audioStreamRef = useRef<MediaStream | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const processorRef = useRef<ScriptProcessorNode | null>(null)

  useEffect(() => {
    return () => {
      disconnect()
    }
  }, [disconnect])

  const connectToGemini = useCallback(async () => {
    try {
      setConnectionStatus('Gemini Live API ì—°ê²° ì¤‘...')

      // ì„¸ì…˜ ID ìƒì„±
      const sessionId = `gemini-${Date.now()}`
      sessionIdRef.current = sessionId
      
      // ì˜¤ë””ì˜¤ ì„¤ì •
      await setupAudioCapture()
      
      // Server-Sent Events ì—°ê²°
      const eventSource = new EventSource(`/api/gemini/live-websocket?sessionId=${sessionId}&sessionNumber=${sessionNumber}`)
      
      eventSource.onopen = () => {
        console.log('Gemini Live SSE ì—°ê²°ë¨')
        setIsConnected(true)
        setConnectionStatus('ìŒì„± ì¸í„°ë·° ì¤€ë¹„ ì™„ë£Œ')
      }

      eventSource.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data)
          handleGeminiResponse(data)
        } catch (error) {
          console.error('SSE ë©”ì‹œì§€ íŒŒì‹± ì˜¤ë¥˜:', error)
        }
      }

      eventSource.onerror = (error) => {
        console.error('SSE ì—°ê²° ì˜¤ë¥˜:', error)
        setConnectionStatus('ì—°ê²° ì˜¤ë¥˜ ë°œìƒ')
        eventSource.close()
        setIsConnected(false)
      }

      // cleanup í•¨ìˆ˜ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ refì— ì €ì¥
      ;(sessionIdRef as any).eventSource = eventSource

    } catch (error) {
      console.error('Gemini ì—°ê²° ì˜¤ë¥˜:', error)
      setConnectionStatus(`ì—°ê²° ì‹¤íŒ¨: ${error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`)
    }
  }, [sessionNumber, setupAudioCapture, handleGeminiResponse])

  const setupAudioCapture = async () => {
    try {
      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        }
      })
      
      audioStreamRef.current = stream

      // AudioContext ì„¤ì •
      const audioContext = new AudioContext({ sampleRate: 16000 })
      audioContextRef.current = audioContext

      const source = audioContext.createMediaStreamSource(stream)
      const processor = audioContext.createScriptProcessor(1024, 1, 1)
      processorRef.current = processor

      processor.onaudioprocess = (event) => {
        if (isRecording && sessionIdRef.current) {
          const inputBuffer = event.inputBuffer.getChannelData(0)
          
          // Float32 to Int16 conversion using utility
          const pcmData = float32ToInt16(inputBuffer)

          // Gemini Live APIì— ì˜¤ë””ì˜¤ ë°ì´í„° ì „ì†¡
          sendAudioToGemini(Array.from(pcmData))
        }
      }

      source.connect(processor)
      processor.connect(audioContext.destination)

      setIsRecording(true)

    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ ì„¤ì • ì˜¤ë¥˜:', error)
      setConnectionStatus('ë§ˆì´í¬ ì ‘ê·¼ ì‹¤íŒ¨')
    }
  }

  const sendAudioToGemini = async (audioData: number[]) => {
    if (!sessionIdRef.current) return
    
    try {
      const response = await fetch('/api/gemini/live-websocket', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          action: 'sendAudio',
          sessionId: sessionIdRef.current,
          audioData
        })
      })
      
      if (!response.ok) {
        console.error('ì˜¤ë””ì˜¤ ì „ì†¡ ì‹¤íŒ¨:', response.statusText)
      }
    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ ì „ì†¡ ì˜¤ë¥˜:', error)
    }
  }


  const handleGeminiResponse = useCallback((response: any) => {
    console.log('Gemini ì‘ë‹µ:', response)
    
    switch (response.type) {
      case 'connected':
        console.log('Gemini Live ì—°ê²°ë¨:', response.message)
        break
        
      case 'text_response':
        if (response.text) {
          const assistantMessage: Conversation = {
            role: 'assistant',
            content: response.text,
            timestamp: new Date(),
            audioComplete: true
          }
          setConversations(prev => [...prev, assistantMessage])
        }
        break
        
      case 'audio_response':
        if (response.audioData) {
          try {
            // Base64 ë””ì½”ë”© í›„ ì˜¤ë””ì˜¤ ì¬ìƒ
            const binaryString = atob(response.audioData)
            const bytes = new Uint8Array(binaryString.length)
            for (let i = 0; i < binaryString.length; i++) {
              bytes[i] = binaryString.charCodeAt(i)
            }
            const int16Data = new Int16Array(bytes.buffer)
            playAudioData(Array.from(int16Data))
            setIsAISpeaking(true)
          } catch (error) {
            console.error('ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜:', error)
          }
        }
        break
        
      case 'raw_response':
        // ì›ì‹œ Gemini ì‘ë‹µ - ë””ë²„ê¹…ìš©
        console.log('ì›ì‹œ Gemini ì‘ë‹µ:', response.data)
        break
        
      case 'error':
        console.error('Gemini ì˜¤ë¥˜:', response.message)
        setConnectionStatus(`ì˜¤ë¥˜: ${response.message}`)
        if (response.details) {
          console.error('ì˜¤ë¥˜ ì„¸ë¶€ì‚¬í•­:', response.details)
        }
        break
        
      case 'heartbeat':
        // ì—°ê²° ìœ ì§€ í™•ì¸
        break
        
      default:
        console.log('ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì‘ë‹µ:', response)
    }
  }, [])

  const playAudioData = (audioData: number[]) => {
    if (!audioContextRef.current) return

    const audioContext = audioContextRef.current
    const int16Data = new Int16Array(audioData)
    
    // 24kHzì—ì„œ 16kHzë¡œ ë¦¬ìƒ˜í”Œë§ (AudioContextê°€ 16kHzì¸ ê²½ìš°)
    // Int16 to Float32 conversion using utility
    const floatData = int16ToFloat32(int16Data)
    
    // ì˜¤ë””ì˜¤ ë²„í¼ ìƒì„± (24kHz ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì¬ìƒ)
    const audioBuffer = audioContext.createBuffer(1, floatData.length, 24000)
    audioBuffer.getChannelData(0).set(floatData)

    const source = audioContext.createBufferSource()
    source.buffer = audioBuffer
    source.connect(audioContext.destination)
    source.start()
    
    // ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
    source.onended = () => {
      setIsAISpeaking(false)
    }
  }


  const disconnect = useCallback(async () => {
    console.log('Gemini ì—°ê²° í•´ì œ ì¤‘...')

    // EventSource í•´ì œ
    const eventSource = (sessionIdRef as any).eventSource
    if (eventSource) {
      eventSource.close()
      ;(sessionIdRef as any).eventSource = null
    }

    // ì„¸ì…˜ ì¢…ë£Œ
    if (sessionIdRef.current) {
      try {
        await fetch('/api/gemini/live-websocket', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            action: 'disconnect',
            sessionId: sessionIdRef.current
          })
        })
      } catch (error) {
        console.error('ì„¸ì…˜ ì¢…ë£Œ ì˜¤ë¥˜:', error)
      }
      sessionIdRef.current = null
    }

    // ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ í•´ì œ
    if (audioStreamRef.current) {
      audioStreamRef.current.getTracks().forEach(track => track.stop())
      audioStreamRef.current = null
    }

    // AudioContext í•´ì œ
    if (audioContextRef.current) {
      audioContextRef.current.close()
      audioContextRef.current = null
    }

    // Processor í•´ì œ
    if (processorRef.current) {
      processorRef.current.disconnect()
      processorRef.current = null
    }

    setIsConnected(false)
    setIsRecording(false)
    setIsAISpeaking(false)
    setConnectionStatus('ì—°ê²° í•´ì œë¨')
  }, [])

  const isMobile = typeof window !== 'undefined' && navigator.userAgent.match(/iPhone|iPad|iPod|Android/i)

  return (
    <div className="bg-white rounded-lg shadow-lg p-4 sm:p-6">
      <div className="mb-4 sm:mb-6">
        <h3 className="text-lg sm:text-xl font-semibold text-gray-800 mb-2">
          ğŸ¤ Gemini Live ì‹¤ì‹œê°„ ìŒì„± ì¸í„°ë·°
        </h3>
        <p className="text-sm sm:text-base text-gray-600 mb-2">{connectionStatus}</p>
        <p className="text-xs text-gray-500">
          Googleì˜ ìµœì‹  Gemini 2.5 Flash ëª¨ë¸ê³¼ ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì„¸ìš”
        </p>
        {isMobile && (
          <p className="text-xs sm:text-sm text-amber-600 mt-2">
            ğŸ“± ëª¨ë°”ì¼ í™˜ê²½ì…ë‹ˆë‹¤. Chrome ë˜ëŠ” Safari ë¸Œë¼ìš°ì € ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
          </p>
        )}
      </div>

      {/* ì—°ê²° ë²„íŠ¼ */}
      <div className="flex justify-center mb-4 sm:mb-6 space-x-2 sm:space-x-4">
        {!isConnected ? (
          <button
            onClick={connectToGemini}
            className="px-4 py-2 sm:px-6 sm:py-3 bg-gradient-to-r from-blue-600 to-purple-600 text-white text-sm sm:text-base rounded-lg hover:from-blue-700 hover:to-purple-700 transition font-medium shadow-lg"
          >
            ğŸ¤ Gemini Live ì‹¤ì‹œê°„ ëŒ€í™” ì‹œì‘
          </button>
        ) : (
          <button
            onClick={disconnect}
            className="px-4 py-2 sm:px-6 sm:py-3 bg-red-600 text-white text-sm sm:text-base rounded-lg hover:bg-red-700 transition font-medium"
          >
            ğŸ›‘ ì‹¤ì‹œê°„ ëŒ€í™” ì¢…ë£Œ
          </button>
        )}
      </div>

      {/* ìŒì„± ìƒíƒœ í‘œì‹œ */}
      {isConnected && (
        <div className="text-center mb-4 sm:mb-6">
          <div className="space-y-2">
            <div className={`inline-flex items-center px-4 py-2 rounded-full ${
              isRecording ? 'bg-red-100 text-red-800' : 'bg-gray-100 text-gray-600'
            }`}>
              <div className={`w-3 h-3 rounded-full mr-2 ${
                isRecording ? 'bg-red-500 animate-pulse' : 'bg-gray-400'
              }`} />
              {isRecording ? 'ğŸ¤ ë§ì”€í•˜ê³  ê³„ì‹­ë‹ˆë‹¤...' : 'ğŸ§ Geminiê°€ ë“£ê³  ìˆìŠµë‹ˆë‹¤'}
            </div>
            
            {isAISpeaking && (
              <div className="inline-flex items-center px-4 py-2 rounded-full bg-blue-100 text-blue-800">
                <div className="w-3 h-3 rounded-full mr-2 bg-blue-500 animate-pulse" />
                ğŸ—£ï¸ Geminiê°€ ì‘ë‹µí•˜ê³  ìˆìŠµë‹ˆë‹¤...
              </div>
            )}
          </div>
        </div>
      )}

      {/* í˜„ì¬ AI ì‘ë‹µ í‘œì‹œ */}
      {currentTranscript && (
        <div className="mb-4 p-4 bg-blue-50 rounded-lg border-l-4 border-blue-400">
          <p className="text-blue-800 font-medium">Gemini (ì‹¤ì‹œê°„):</p>
          <p className="text-blue-700 mt-1">{currentTranscript}</p>
        </div>
      )}

      {/* ëŒ€í™” ê¸°ë¡ */}
      <div className="max-h-96 overflow-y-auto space-y-4">
        {conversations.map((conv, index) => (
          <div
            key={index}
            className={`p-4 rounded-lg ${
              conv.role === 'assistant' 
                ? 'bg-blue-50 border-l-4 border-blue-400' 
                : 'bg-green-50 border-l-4 border-green-400'
            }`}
          >
            <div className="flex justify-between items-start mb-2">
              <span className={`font-medium ${
                conv.role === 'assistant' ? 'text-blue-800' : 'text-green-800'
              }`}>
                {conv.role === 'assistant' ? 'ğŸ¤– Gemini ì¸í„°ë·°ì–´' : 'ğŸ‘¤ ì•„ë²„ë‹˜'}
              </span>
              <span className="text-xs text-gray-500">
                {conv.timestamp.toLocaleTimeString()}
              </span>
            </div>
            <p className="text-gray-800 whitespace-pre-wrap">{conv.content}</p>
          </div>
        ))}

        {conversations.length === 0 && isConnected && (
          <div className="text-center text-gray-500 py-8">
            <p>Gemini Live ì¸í„°ë·°ê°€ ê³§ ì‹œì‘ë©ë‹ˆë‹¤...</p>
            <p className="text-sm mt-2">ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•˜ê³  Geminiì˜ ì§ˆë¬¸ì„ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.</p>
          </div>
        )}
      </div>

      {/* ì‚¬ìš© íŒ */}
      {isConnected && (
        <div className="mt-4 p-3 bg-gradient-to-r from-blue-50 to-purple-50 rounded-lg text-sm text-gray-600 border border-blue-200">
          <h4 className="font-medium text-gray-800 mb-2">ğŸ’¡ Gemini Live ì‹¤ì‹œê°„ ëŒ€í™” ê°€ì´ë“œ:</h4>
          <ul className="space-y-1 text-xs">
            <li>ğŸ¯ <strong>ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ì„¸ìš”</strong> - ë§ˆì¹˜ ì¹œêµ¬ì™€ ëŒ€í™”í•˜ë“¯ í¸ì•ˆí•˜ê²Œ</li>
            <li>ğŸª <strong>ì‹¤ì‹œê°„ ì‘ë‹µ</strong> - AIê°€ ì¦‰ì‹œ ìŒì„±ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤</li>
            <li>ğŸŒ <strong>í•œêµ­ì–´ ì™„ë²½ ì§€ì›</strong> - ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ëŒ€í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤</li>
            <li>ğŸ§ <strong>í—¤ë“œí° ê¶Œì¥</strong> - ì—ì½” ë°©ì§€ë¥¼ ìœ„í•´ í—¤ë“œí° ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤</li>
            <li>âœ¨ <strong>í’ë¶€í•œ ì´ì•¼ê¸°</strong> - ìì„¸í•˜ê²Œ ë§ì”€í•˜ì‹¤ìˆ˜ë¡ ë” ì¢‹ì€ ìì„œì „ì´ ì™„ì„±ë©ë‹ˆë‹¤</li>
          </ul>
        </div>
      )}
    </div>
  )
}